Fused Attention
=======

Attention (including MHA, GQA, MHA) is one of the key parts of transformers and also the bottleneck in many scenarios. To implement various optimizations, a fused attention layer and corresponding utilities for the customized KV-cache it uses are implemented.

## KV-cache initialization
The memory for kv-cache is allocated in `/models/model_utils/model_utils.cpp`. As the fused attention implementation requires certain instruction extensions and potentially some other limitations, fused attention is enabled only if `jblas_reordered_attn_fp32_support()` aggress, denoting with `memory_type = NE_TYPE_JBLAS`. Next, `get_batch_kv_elements_from_gpt_params()` will give the sizes (in terms of bytes if fused attention enabled, or in terms elements if fused attention disabled) of k-cache and v-cache respectively for each batch and each layer. The KV-cache is finally prepared with these 2 sizes by creating `ne_new_tensor` inside `model.kv_self.k/.v` or `model.layer[il].k_cache/.v_cache`.

## KV-cache Append
KV-cache is appended every time a new pair of K and V are generated by evaluating inner product for QKV (ne_mul_qkv). The tensor of K and V should be both permuted as `batch x N x head_num x head_size` (stride for the outer 3 dims are flexible). The permuted K/V tensors can then be passed to `ne_flash_attn_update_k/_v`, together with `n_past` as an offset used internally, to concatenate to a view of the cached K/V of the current layer and current batch.

> Note: Currently the K and V tensor to append must be contiguous in memory in the dimension of head size (i.e. `head_dim` in some implementation)

## Fused Attention Computation
With the KV-cache of customized type and layout, the attention can be computed at its best performance. `ne_flash_attn` accepts a Q-tensor of `batch x head_num x N x head_size` (may be permuted and not contiguous on dimensions other than head size) and outputs a result tensor of `batch x N x head_num x head_size` (totally contiguous).
