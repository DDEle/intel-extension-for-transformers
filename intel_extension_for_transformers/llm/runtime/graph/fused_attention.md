Fused Attention
=======

Attention (including MHA, GQA, MHA) is one of the key parts of transformers and also the performance critical in many scenarios. To implement various optimizations, a fused attention layer and corresponding utilities for the customized KV-cache it uses are introduced.

Note, this doc assumes you have the basic knowledge of this cpp graph implementation including `ne_tensor::ne`, `ne_tensor::nb` etc.

## KV-cache initialization
The memory for kv-cache is allocated in `/models/model_utils/model_utils.cpp`. As the fused attention implementation requires certain instruction extensions and potentially some other limitations, fused attention is enabled only if `jblas_reordered_attn_fp32_support()` aggress, denoting with `memory_type = NE_TYPE_JBLAS`. Next, `get_batch_kv_elements_from_gpt_params()` will give the sizes (in terms of bytes if fused attention enabled, or in terms elements if fused attention disabled) of k-cache and v-cache respectively for each batch and each layer. The KV-cache is finally prepared with these 2 sizes by creating `ne_new_tensor` inside `model.kv_self.k/.v` or `model.layer[il].k_cache/.v_cache`.

## KV-cache Append
KV-cache is appended every time a new pair of K and V are generated by evaluating inner product for QKV (ne_mul_qkv). This operation append an additional K/V-tensor on the dimension of sequence length (i.e. resulting `n_past = n_past + N`, where `n_past` is number of previously cached tokens and `N` is the length of current tokens).

The tensor of K and V should be both permuted (with `ne_permute`) to `batch x N x head_num x head_size`. The permuted K/V tensors can then be passed to `ne_flash_attn_update_k()/_v()`, together with `n_past` as an offset in the dimension of sequence length, to concatenate to a "view" of the cached K/V of the current layer and current batch.

> Note: Currently the K and V tensor to append must be contiguous in the dimension of head size (i.e. `head_dim` in some implementations). The strides (`ne_tensor::nb`) of the other 3 dimensions are configurable.

## Fused Attention Computation
With the KV-cache of customized type and layout, the attention can be computed at its best performance. `ne_flash_attn` accepts a Q-tensor of `batch x head_num x N x head_size` and outputs a result tensor of `batch x N x head_num x head_size` (totally contiguous).

> Node: similar to the append operation, Q-tensor must be contiguous in the dimension of head and its strides (`ne_tensor::nb`) of the other 3 dimensions are configurable.

## Supported Models
Fused attention is designed to be able to easily support various models:

| Model | Attention Type & Support |
|---|:---:|
|[LLaMA-7B](https://huggingface.co/decapoda-research/llama-7b-hf), [LLaMA-13B](https://huggingface.co/decapoda-research/llama-13b-hf), [LLaMA2-7B](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf), [LLaMA2-13B](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) | MHA âœ… |
|[LLaMA2-70B](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)| GQA ðŸš§ |
|[GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6b)| MHA âœ… |
|[GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b), [Dolly-v2-3B](https://huggingface.co/databricks/dolly-v2-3b) | MHA ðŸš§ |
|[MPT-7B](https://huggingface.co/mosaicml/mpt-7b), [MPT-30B](https://huggingface.co/mosaicml/mpt-30b)| MHA with [ALiBi](https://arxiv.org/abs/2108.12409) ðŸš§ |
|[Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b)| MQA, GQA âœ… |
|[BLOOM-7B](https://huggingface.co/bigscience/bloomz-7b1)| MHA with [ALiBi](https://arxiv.org/abs/2108.12409) ðŸš§ |
|[OPT-125m](https://huggingface.co/facebook/opt-125m), [OPT-350m](https://huggingface.co/facebook/opt-350m), [OPT-1.3B](https://huggingface.co/facebook/opt-1.3b), [OPT-13B](https://huggingface.co/facebook/opt-13b)| MHA ðŸš§ |
|[ChatGLM-6B](https://huggingface.co/THUDM/chatglm-6b), [ChatGLM2-6B](https://huggingface.co/THUDM/chatglm2-6b)| MHA ðŸš§ï¼Œ GQA âœ… |
|[StarCoder-1B](https://huggingface.co/bigcode/starcoderbase-1b), [StarCoder-3B](https://huggingface.co/bigcode/starcoderbase-3b), [StarCoder-15.5B](https://huggingface.co/bigcode/starcoder)| MHA ðŸš§ |

## Limitations
Currently the fused attention is only enabled when compiling the llm runtime with GCC13 and running it on platforms with the IntelÂ® AMX Instructions.

## Tips for parallelism
Thanks to the mathematical nature of attention, one can simply parallel the whole kv-cache operations and fused attention on commonly-parallelizable dimensions. Just pass each part to every KV-cache operations (and merge them together if needed).
