Fused Attention
=======

Attention (including MHA, GQA, MHA) is one of the key parts of transformers and also the bottleneck in many scenarios. To implement various optimizations, a fused attention layer and corresponding utilities for the customized KV-cache it uses are introduced.

Note, this doc assumes you have the basic knowledge of this cpp graph implementation including `ne_tensor::ne`, `ne_tensor::nb` etc.

## KV-cache initialization
The memory for kv-cache is allocated in `/models/model_utils/model_utils.cpp`. As the fused attention implementation requires certain instruction extensions and potentially some other limitations, fused attention is enabled only if `jblas_reordered_attn_fp32_support()` aggress, denoting with `memory_type = NE_TYPE_JBLAS`. Next, `get_batch_kv_elements_from_gpt_params()` will give the sizes (in terms of bytes if fused attention enabled, or in terms elements if fused attention disabled) of k-cache and v-cache respectively for each batch and each layer. The KV-cache is finally prepared with these 2 sizes by creating `ne_new_tensor` inside `model.kv_self.k/.v` or `model.layer[il].k_cache/.v_cache`.

## KV-cache Append
KV-cache is appended every time a new pair of K and V are generated by evaluating inner product for QKV (ne_mul_qkv). This operation append an additional K/V-tensor on the dimension of `n_past` (or the dimension of sequence length) (i.e. resulting `n_past = n_past + N`).

The tensor of K and V should be both permuted (with `ne_permute`) to `batch x N x head_num x head_size`. The permuted K/V tensors can then be passed to `ne_flash_attn_update_k()/_v()`, together with `n_past` as an offset in the dimension of sequence length, to concatenate to a "view" of the cached K/V of the current layer and current batch.

> Note: Currently the K and V tensor to append must be contiguous in the dimension of head size (i.e. `head_dim` in some implementation). The strides (`ne_tensor::nb`) of the other 3 dimensions are configurable.

## Fused Attention Computation
With the KV-cache of customized type and layout, the attention can be computed at its best performance. `ne_flash_attn` accepts a Q-tensor of `batch x head_num x N x head_size` and outputs a result tensor of `batch x N x head_num x head_size` (totally contiguous).

> Node: similar to the append operation, Q-tensor must be contiguous in the dimension of head and its strides (`ne_tensor::nb`) of the other 3 dimensions are configurable.

## More KV-cache Operations is Being Implemented...
While we are working on extra KV-cache operations, one can simply parallel the whole kv-cache operations and fused attention on commonly-parallelizable dimensions. Just pass each part to every KV-cache operations (and join them together if needed).
